{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0465ca",
   "metadata": {},
   "source": [
    "# Scraping ANS: Data collection \n",
    "\n",
    "The objective of this notebook is to scrape the ANS (de Algemene Nederlandse Spraakkunst) (ANS | e-ANS, z.d.) website for minimal pairs in order to create a challenge set to evaluate Dutch language models on their linguistic knowledge.\n",
    "\n",
    "The ANS is the standard reference work in which the grammatical rules of Dutch are described. The grammatical rules that are described include examples of correct and incorrect usage of those grammatical rules. Some sentences are classed into one of the following groups: informeel, formeel, uitgesloten and twijfelachtig.\n",
    "\n",
    "We employ web scraping (extracting content and data from websites) and crawling (using URLs to get more information) to save the sentences that are part of one of these classes to a Data Frame in order to be used for future research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b7aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import argparse\n",
    "from requests import get, RequestException\n",
    "from contextlib import closing\n",
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99fcac",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "I will be utelising functions created by professors of the minor Artificial Intelligence at the UvA that will enable me to effectively scrape and crawl the ANS website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce45fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_get(url, max_retries=3):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    Retry up to `max_retries` times on HTTP errors with a 1-second delay.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'\n",
    "        }\n",
    "        for _ in range(max_retries):\n",
    "            with closing(get(url, stream=True, headers=headers)) as resp:\n",
    "                if is_good_response(resp):\n",
    "                    return resp.content\n",
    "                else:\n",
    "                    print(f\"Recieved a HTTP {resp.status_code} ERROR for {url}.\")\n",
    "                    print(\"Retrying in 1 second...\")\n",
    "                    time.sleep(1)\n",
    "        print(\"-------------------------------------\")\n",
    "        print(f\"Retrieving {url} FAILED. Visit this URL in your browser to confirm correctness.\")\n",
    "        print(\"-------------------------------------\")\n",
    "        return None\n",
    "    except RequestException as e:\n",
    "        print(f\"The following error occurred during HTTP GET request to {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def url_to_filename(url, folder):\n",
    "    \"\"\"\n",
    "    Transforms an URL string to a folder/filename string by replacing slashes\n",
    "    with underscores.\n",
    "    \"\"\"\n",
    "    return os.path.join(folder, f\"{url.replace('https://', '').replace('/', '_')}.html\")\n",
    "\n",
    "\n",
    "def save_to_html(url, content, filename):\n",
    "    \"\"\"\n",
    "    Saves the page content to an HTML file in the 'data' folder.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def retrieve_and_store_page_content(url, folder):\n",
    "    \"\"\"\n",
    "    Retrieves the page content for a single URL and writes it to the folder data.\n",
    "    \"\"\"\n",
    "    filename = url_to_filename(url, folder)\n",
    "\n",
    "    # Check if the file already exists and has content\n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
    "        return\n",
    "\n",
    "    content = simple_get(url)\n",
    "    save_to_html(url, content, filename)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def get_page_contents_multiprocess(url_list, processes=20, folder='webpages'):\n",
    "    \"\"\"\n",
    "    Retrieves the page content for a list of URLs using multiprocessing.\n",
    "    By default writes the content to HTML files in the 'webpages' folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the folder exists, and create it if needed\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    with multiprocessing.Pool(processes=20) as pool:\n",
    "        pool.map(partial(retrieve_and_store_page_content, folder=folder), url_list)\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns true if the response seems to be HTML, false otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "            and content_type is not None\n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def read_page_content_from_disk(url, folder='webpages'):\n",
    "    \"\"\"\n",
    "    Reads the page content from a file on disk.\n",
    "    \"\"\"\n",
    "    filename = url_to_filename(url, folder)\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        print(f\"Have you scraped this URL: {url}?\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "        if len(content) == 0:\n",
    "            print(f\"{filename} is empty. Something went wrong during crawling!\")\n",
    "\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1533f3",
   "metadata": {},
   "source": [
    "### Splitting the sentences\n",
    "\n",
    "When extracting the example sentences from the data, we find that in some cases the format is not clear. Sometimes there are an excess amount of white spaces in the string we extract, or an unnecessary new line character. Below is an example of one of the sentences.\n",
    "\n",
    "     De uitspraak van de\n",
    "     \n",
    "           w\n",
    "     \n",
    "We, of course, want to modify the text with the purpose of creating a clear sentence that can easily be comprehended and searched for in a data frame. I have defined a function `split_string()` that takes a string, splits it and joins it back together again with single white spaces in between each word. \n",
    "\n",
    "Another issue that will present itself during crawling of the data; we will extract sentences with (again) too many white spaces and sometimes an unwanted new line character, but also the class label stuck to the end of the sentence as seen in the example below.\n",
    "\n",
    "     Zonder een bril\n",
    "     \n",
    "           ziet hij bijna niets.uitgesloten\n",
    "           \n",
    "I have defined a function `split_sentence()` that takes a string, splits it just like the function `split_string()` and removed the label at the end of the sentence. This function, however, does not work properly. So it will not be used in this notebook. I have left it in so it can be rewritten and fixed in order to be used properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300a6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(string):\n",
    "    # Split the sentence, get list of all words in string\n",
    "    new_sentence = string.split()\n",
    "    \n",
    "    # Join words from list back together with single whitespaces\n",
    "    joined_string = ' '.join(new_sentence)\n",
    "    return joined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d772318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string, removes whitespaces, newlines and label, and returns the newly formatted string.\n",
    "    :Param\n",
    "        - sentence: Sentence that needs to be split (string)\n",
    "    Returns:\n",
    "        - split_sentence: Sentence without extra whitespaces, newlines and label\n",
    "    \"\"\"\n",
    "    # Split the sentence, get list of all words in string\n",
    "    new_sentence = sentence.split()\n",
    "    \n",
    "    # Join words from list back together with single whitespaces\n",
    "    joined_string = ' '.join(new_sentence)\n",
    "    \n",
    "    last_word = new_sentence[-1]\n",
    "    \n",
    "    # Loop over characters in last element of list (the element with the label stuck after the punctuation)\n",
    "    for char in last_word:\n",
    "        \n",
    "        # Check if character is period\n",
    "        if char == '.':\n",
    "            \n",
    "            # Split string at the period and select the 0th element from list\n",
    "            my_string_new = joined_string.split('.')[0]\n",
    "            \n",
    "            # Create split sentence with the punctuation back at the end\n",
    "            clean_sentence = f\"{my_string_new}.\"\n",
    "            return clean_sentence\n",
    "            \n",
    "        elif char == '?':\n",
    "            my_string_new = joined_string.split('?')[0]\n",
    "            clean_sentence = f\"{my_string_new}?\"\n",
    "            return clean_sentence\n",
    "            \n",
    "        elif char == '!':\n",
    "            my_string_new = joined_string.split('!')[0]\n",
    "            clean_sentence = f\"{my_string_new}!\"\n",
    "            return clean_sentence\n",
    "        \n",
    "    return joined_string\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53720935",
   "metadata": {},
   "source": [
    "### Scraping the ANS\n",
    "\n",
    "Firstly, we want to initialize the url and use the helpers function `simple_get()` to get the domain we want to scrape. Hereafter, we define a function `scrape_data()` to extract the information from the menu bar on the ANS website. For this part of the scraping, we are concerning ourselves with the extraction of the following data:\n",
    "\n",
    "1. Subject (i.e. 'De klank', 'Het woord', etc.)\n",
    "2. Section (the numeric section of the sub-subject, i.e. 1, 1.2.1, etc.)\n",
    "3. Sub-section (i.e. 'De klankleer van het Nederlands', 'Soorten werkwoorden', etc.)\n",
    "4. URL (the URLs of those subsections that will be crawled to extract further information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2737d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://e-ans.ivdnt.org/'\n",
    "page = simple_get(url)\n",
    "dom = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c860044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(dom):\n",
    "    '''\n",
    "    :Param\n",
    "        dom: html website voor ANS\n",
    "    Returns:\n",
    "        df: a data frame containing information of the subjects discussed on the website\n",
    "            - Subject\n",
    "            - Section\n",
    "            - Sub-subject\n",
    "            - URL\n",
    "    '''\n",
    "    data = []\n",
    "    subjects = []\n",
    "    sections = []\n",
    "    sub_subjects = []\n",
    "    urls = []\n",
    "\n",
    "    # Create lists containing strings of section number corresponding to the subjects\n",
    "    woord_sections = ['2','3','4','5','6','7','8','9','10','11','12']\n",
    "    constituent_sections = ['13','14','15','16','17','18']\n",
    "    zin_sections = ['19','20','21','22','23']\n",
    "\n",
    "\n",
    "    # Find all subjects and save them to list\n",
    "    for subject in dom.find_all(\"div\", class_=\"nav-chapters\"):\n",
    "        subject_ = subject.span.text\n",
    "        subjects.append(subject_)\n",
    "\n",
    "    # Find sub-subject information\n",
    "    for item in dom.find_all(\"div\", class_=\"nav-ch-header\"):\n",
    "\n",
    "        # Save section to list\n",
    "        section = item.find(\"span\", class_=\"idx\").text\n",
    "        sections.append(section)\n",
    "\n",
    "        # Find title for sub-subject and add to list\n",
    "        sub_subject_ = item.find(\"span\", class_=\"ttl\").text\n",
    "        sub_subject = split_string(sub_subject_)\n",
    "\n",
    "        # Find url for sub-subject and add to list\n",
    "        url_ = item.a['href']\n",
    "        url = f'https://e-ans.ivdnt.org{url_}'\n",
    "\n",
    "        # Determine main_subject based on which section the \n",
    "        if section.split('.')[0] == '1':\n",
    "            main_subject = subjects[0]\n",
    "        elif section.split('.')[0] in woord_sections:\n",
    "            main_subject = subjects[1]\n",
    "        elif section.split('.')[0] in constituent_sections:\n",
    "            main_subject = subjects[2]\n",
    "        elif section.split('.')[0] in zin_sections:\n",
    "            main_subject = subjects[3]\n",
    "        else:\n",
    "            main_subject = subjects[4]\n",
    "\n",
    "        # Append the information to their respective keys in a dictionary and\n",
    "        # append that to the list 'data'\n",
    "        data.append(\n",
    "                {\n",
    "                    'Subject': main_subject,\n",
    "                    'Section': section,\n",
    "                    'Sub-subject': sub_subject,\n",
    "                    'URL': url,\n",
    "                }\n",
    "                    )\n",
    "\n",
    "    # Create a dataframe from the listed dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92e8ab",
   "metadata": {},
   "source": [
    "### Crawling the ANS\n",
    "\n",
    "Now that we have a dataframe containting the preliminary information on the ANS website, it will be possible the crawl the URLs and extract the sentences that are classed as informeel, formeel, uitgesloten or twijfelachtig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7678f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wrong_sentence(listed_example, ilex):\n",
    "    # Extract label\n",
    "    label = listed_example.find(\"span\", class_=ilex).text\n",
    "\n",
    "    # Extract the example sentence\n",
    "    wrong_sentence = listed_example.text\n",
    "    stripped_wrong_sentence = split_sentence(wrong_sentence)\n",
    "    labeled_example = True\n",
    "    \n",
    "    return label, stripped_wrong_sentence, labeled_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a9323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_data(df, url_list):\n",
    "    \"\"\"\n",
    "    :Param\n",
    "        df: pandas data frame containing information on the ANS website\n",
    "            - Subject\n",
    "            - Section\n",
    "            - Sub-subject\n",
    "            - URL\n",
    "        url_list: a list with the urls we want to crawl\n",
    "    Returns:\n",
    "        df: the input pandas data frame with two additional columns containing the class and sentence\n",
    "            With columns:\n",
    "            - Subject\n",
    "            - Section\n",
    "            - Sub-subject\n",
    "            - Class\n",
    "            - Sentence\n",
    "            - URL\n",
    "    \"\"\"\n",
    "    data_complete = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    row_index = 0\n",
    "    count_examples = 0\n",
    "    prev_count_examples = 0\n",
    "    labeled_example = False\n",
    "    correct_example = False\n",
    "    \n",
    "    for url in url_list:        \n",
    "        # Load and read webpage\n",
    "        html = read_page_content_from_disk(url)\n",
    "        ans_item = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find all listed examples in each webpage (example 'a', example 'b', etc.)\n",
    "        compare_examples = ans_item.find_all(\"div\", class_=\"relatedList\")\n",
    "        \n",
    "        # Continue if there are examples in webpage\n",
    "        if len(compare_examples) > 0:\n",
    "        \n",
    "            # Loop over listed examples\n",
    "            for example in compare_examples:\n",
    "                count_examples+=1\n",
    "                \n",
    "                # Find all individual examples from the ones listed\n",
    "                listed_examples = example.find_all(\"span\", class_=\"ilex-wg\")\n",
    "                \n",
    "                # Loop over individual examples\n",
    "                for listed_example in listed_examples:\n",
    "\n",
    "                    # Find if sentence has a label\n",
    "                    if listed_example.find(\"span\", class_=\"ilex-judgment\") is not None:\n",
    "                        label, stripped_wrong_sentence, labeled_example = extract_wrong_sentence(listed_example, \"ilex-judgment\")\n",
    "\n",
    "                    elif listed_example.find(\"span\", class_=\"ilex-label\") is not None:\n",
    "                        label, stripped_wrong_sentence, labeled_example = extract_wrong_sentence(listed_example, \"ilex-label\")\n",
    "\n",
    "                    # If sentence has no label, save sentence as correct_sentence\n",
    "                    else:\n",
    "                        correct_sentence = listed_example.text\n",
    "                        \n",
    "                        # Clean example sentences\n",
    "                        stripped_correct_sentence = split_string(correct_sentence)\n",
    "                        correct_example = True\n",
    "                        \n",
    "                    # Is there is no correct sentence, save \"NA to dataframe\"\n",
    "                    if (correct_example == False) and (prev_count_examples != count_examples):\n",
    "                        stripped_correct_sentence = \"NA\"\n",
    "\n",
    "                    if labeled_example == True:\n",
    "                        # Append data to dictionary in list\n",
    "                        data_complete.append(\n",
    "                                        {\n",
    "                                            'Subject': df.loc[row_index, 'Subject'],\n",
    "                                            'Section': df.loc[row_index, 'Section'],\n",
    "                                            'Sub-subject':  df.loc[row_index, 'Sub-subject'],\n",
    "                                            'Correct sentence': stripped_correct_sentence,\n",
    "                                            'Wrong sentence': stripped_wrong_sentence,\n",
    "                                            'Label': label,\n",
    "                                            'URL': url,\n",
    "                                        }\n",
    "                                            )\n",
    "                        \n",
    "                        labeled_example = False\n",
    "                        correct_example = False\n",
    "                        \n",
    "                    prev_count_examples = count_examples\n",
    "\n",
    "        row_index+=1\n",
    "        \n",
    "    df = pd.DataFrame(data_complete)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5729c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_csv(df, folder, file_name):\n",
    "    \"\"\"\n",
    "    Takes a data frame, converts it to a csv file, names it as specified by user \n",
    "    and saves it to the specified folder \n",
    "    :Param\n",
    "        - df: a Pandas dataframe\n",
    "        - folder: the name of the folder where we want to save the csv file\n",
    "        - file_name: the file name of the csv file\n",
    "    \"\"\"\n",
    "    # Get current working directory\n",
    "    curr_wd = os.getcwd()\n",
    "    \n",
    "    # Check if the folder exists, and create it if needed\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # Define the folder path\n",
    "    folder_path = f\"{curr_wd}/{folder}\"\n",
    "\n",
    "    # Join the folder path and file name to get the complete file path\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cc3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_webpages(url_list, folder):\n",
    "    \"\"\"\n",
    "    Takes a list of URLs and saves each as a html file to the specified folder\n",
    "    :Param\n",
    "        - url_list: list of URLs\n",
    "        - folder: name of folder we want to save each individual URL to\n",
    "    \"\"\"\n",
    "    # Get current working directory\n",
    "    curr_wd = os.getcwd()\n",
    "    \n",
    "    for url in url_list:\n",
    "        # Define the folder path\n",
    "        folder_path = f\"{curr_wd}/{folder}\"\n",
    "\n",
    "        # Check if the folder exists, if not, create it\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Call on helpers function to retreive and store the content of the url\n",
    "        retrieve_and_store_page_content(url, folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705a9b7",
   "metadata": {},
   "source": [
    "### Main function\n",
    "\n",
    "In the main function, we call on the functions `scrape_data()` and `crawl_data()` and save the resulting dataframes using the `save_df_as_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a3e2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Call on 'scrape_data' function to extract the preliminary information \n",
    "    df_menu = scrape_data(dom)\n",
    "\n",
    "    # Save the data frame with preliminary data as a csv file to the 'data' folder \n",
    "    save_df_as_csv(df_menu, 'data', 'preliminary_data.csv')\n",
    "\n",
    "    # Get URLs from the data frame column ['URL']\n",
    "    url_list = df_menu['URL'].tolist()\n",
    "\n",
    "    # Call on 'save_webpages()' to get the content of each url and save it to the 'webpages' folder \n",
    "    save_webpages(url_list, 'webpages')\n",
    "    \n",
    "    # Call on 'crawl_data()' to get the example sentences and their label\n",
    "    complete_data = crawl_data(df_menu, url_list)\n",
    "\n",
    "    # Dave the data frame with the complete results as a csv file to the 'data' folder\n",
    "    save_df_as_csv(complete_data, 'data', 'complete_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3d344",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this project we show how scraping can be utilised for the extraction of important information. There are, however, a few shortcomings of this scraping project that should be discussed. \n",
    "\n",
    "Firstly, in some columns, the content is printed in a way that is not ideal. For instance:\n",
    "\n",
    "     Zonder een bril\n",
    "     \n",
    "           ziet hij bijna niets.uitgesloten\n",
    "\n",
    "As we can see, there is a new line in the middle of the sentence, too many whitespaces and the label is attached to the end of the sentence. To rectify this, I defined a function `split_sentence()` which should take one such string and return a comprehendible version, like this:\n",
    "\n",
    "    Zonder een bril ziet hij bijna niets.\n",
    "    \n",
    "This function sadly does not work when called on in the `crawl_data()` function. I decided to define another function `split_string()` that only rids the string of unnecessary white spaces and new line characters. It does not however, strip the label from the end of the sentence. I left the function `split_sentence()` in this notebook because I think the function would be helpfull if it worked and it would be worth it to rewrite it so it does work.\n",
    "\n",
    "One of the main limitations of this code is that it is very slow; specifically the `crawl_data()` function. This is mainly due to the large amount of webpages the function loops over. For each webpage, the function finds all examples given and determines if they have a label. If so, it saves the label along with the sentence to a new data frame. This is a tedious process and takes a lot of time. I believe there would likely be a way to optimize this code with the aim to minimize the time complexity (Simplilearn, 2023). I, however, do not have the skill set to be able to do this in the timeframe to complete this project. \n",
    "\n",
    "In conclusion, I believe this project provides a stepping stone to successfully extract example sentences from the ANS website. There is much that can still be improved but, as has been proven with this project, it is definitely possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f46ba0",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "ANS | e-ANS. (z.d.). \n",
    "    https://e-ans.ivdnt.org/\n",
    "\n",
    "Simplilearn. (2023, 6 november). Introduction to Big O Notation in Data Structure. Simplilearn.com. https://www.simplilearn.com/big-o-notation-in-data-structure-article#:~:text=Big%20O%20Notation%20in%20Data%20Structure%20is%20used%20to%20express,algorithm%20for%20an%20input%20value.\n",
    "\n",
    "Minor Artificial Intelligence, Universiteit van Amsterdam\n",
    "    https://www.proglab.nl/programs/minai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
